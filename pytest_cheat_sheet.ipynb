{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6165d72",
   "metadata": {},
   "source": [
    "# Run Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581ad98",
   "metadata": {},
   "source": [
    "**Run Tests:**\n",
    "\n",
    "<code>py.test</code>\n",
    "\n",
    "**To run tests only from a specific file, we can use py.test <filename>**\n",
    "    \n",
    "<code>py.test test_sample1.py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62d2a4",
   "metadata": {},
   "source": [
    "## Run tests by substring matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a9285",
   "metadata": {},
   "source": [
    "<code>py.test -k method1 -v\n",
    "-k <expression> is used to represent the substring to match\n",
    "-v increases the verbosity</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1780123",
   "metadata": {},
   "source": [
    "<code>py.test -k method -v - will run all the four methods\n",
    "py.test -k methods -v – will not run any test as there is no test name matches the substring 'methods'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cee5ab",
   "metadata": {},
   "source": [
    "## Run tests by markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84cc71",
   "metadata": {},
   "source": [
    "<code>@pytest.mark.name.</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f940bc5",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "<code>\n",
    "import pytest\n",
    "@pytest.mark.set1\n",
    "def test_file1_method1():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "\tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "</code>\n",
    "<code>\n",
    "@pytest.mark.set2\n",
    "def test_file1_method2():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4def3",
   "metadata": {},
   "source": [
    "**RUN:**\n",
    "\n",
    "\n",
    "<code>py.test -m <name>\n",
    "-m 'name' mentions the marker name\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269cd09",
   "metadata": {},
   "source": [
    "# Run Tests in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba32def",
   "metadata": {},
   "source": [
    "<code>pip install pytest-xdist</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912eb26b",
   "metadata": {},
   "source": [
    "**Run:**\n",
    "\n",
    "<code>py.test -n 4</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a4ffa",
   "metadata": {},
   "source": [
    "# Pytest Fixtures (usually used to fix sql connection)\n",
    "Fixtures are used when we want to run some code before every test method. So instead of repeating the same code in every test we define fixtures. Usually, fixtures are used to initialize database connections, pass the base , etc\n",
    "\n",
    "A method is marked as a Pytest fixture by marking with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0a595",
   "metadata": {},
   "source": [
    "<code>@pytest.fixture</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92204d6",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "<code>import pytest\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "\taa=25\n",
    "\tbb =35\n",
    "\tcc=45\n",
    "\treturn [aa,bb,cc]\n",
    "</code>\n",
    "\n",
    "<code>def test_comparewithAA(supply_AA_BB_CC):\n",
    "\tzz=35\n",
    "\tassert supply_AA_BB_CC[0]==zz,\"aa and zz comparison failed\"</code>\n",
    "    \n",
    "\n",
    "<code>def test_comparewithBB(supply_AA_BB_CC):\n",
    "\tzz=35\n",
    "\tassert supply_AA_BB_CC[1]==zz,\"bb and zz comparison failed\"</code>\n",
    "    \n",
    "\n",
    "<code>def test_comparewithCC(supply_AA_BB_CC):\n",
    "\tzz=35\n",
    "\tassert supply_AA_BB_CC[2]==zz,\"cc and zz comparison failed\"</code>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4453ea9",
   "metadata": {},
   "source": [
    "**Run test:**\n",
    "\n",
    "<code>py.test test_basic_fixture</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7e166",
   "metadata": {},
   "source": [
    "**The fixture method has a scope only within that test file it is defined. If we try to access the fixture in some other test file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db631b",
   "metadata": {},
   "source": [
    "# conftest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63c3b8",
   "metadata": {},
   "source": [
    "**To use the same fixture against multiple test files, we will create fixture methods in a file called conftest.py.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfe251",
   "metadata": {},
   "source": [
    "<code> py.test -k test_comparewith -v</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2ab97",
   "metadata": {},
   "source": [
    "# Pytest Parameterized Test\n",
    "\n",
    "The purpose of parameterizing a test is to run a test against multiple sets of arguments. We can do this by\n",
    "\n",
    "<code>@pytest.mark.parametrize.</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f3eb7",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "<code>import pytest\n",
    "@pytest.mark.parametrize(\"input1, input2, output\",[(5,5,10),(3,5,12)])</code>\n",
    "    \n",
    "<code>def test_add(input1, input2, output):\n",
    "\tassert input1+input2 == output,\"failed\"</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f960fa",
   "metadata": {},
   "source": [
    "# Pytest Xfail/ Skip Tests\n",
    "\n",
    "There will be some situations where we don’t want to execute a test, or a test case is not relevant for a particular time. In those situations, we have the option to Xfail the test or skip the tests\n",
    "\n",
    "The xfailed test will be executed, but it will not be counted as part failed or passed tests. There will be no traceback displayed if that test fails. We can xfail tests using\n",
    "\n",
    "<code>@pytest.mark.xfail</code>.\n",
    "\n",
    "Skipping a test means that the test will not be executed. We can skip tests using\n",
    "\n",
    "<code>@pytest.mark.skip.</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4eb090",
   "metadata": {},
   "source": [
    "**Run:**\n",
    "\n",
    "<code>py.test test_addition.py -v</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6aabc",
   "metadata": {},
   "source": [
    "# Results XML (for CI)\n",
    "\n",
    "We can create test results in XML format which we can feed to Continuous Integration servers for further processing and so. This can be done by\n",
    "\n",
    "<code>py.test test_sample1.py -v –junitxml=”result.xml”</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c55701",
   "metadata": {},
   "source": [
    "# Framework Testing an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9d041",
   "metadata": {},
   "source": [
    "## Create a conftest.py\n",
    "\n",
    "conftest.py – have a fixture which will supply base url for all the test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def supply_url():\n",
    "\treturn \"https://reqres.in/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4884f0a",
   "metadata": {},
   "source": [
    "## create a file with the test methods\n",
    "\n",
    "**test_list_user.py** – contains the test methods for listing valid and invalid users\n",
    "\n",
    "- **test_list_valid_user** tests for valid user fetch and verifies the response\n",
    "- **test_list_invaliduser** tests for invalid user fetch and verifies the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import requests\n",
    "import json\n",
    "\n",
    "@pytest.mark.parametrize(\"userid, firstname\",[(1,\"George\"),(2,\"Janet\")])\n",
    "def test_list_valid_user(supply_url,userid,firstname):\n",
    "\turl = supply_url + \"/users/\" + str(userid)\n",
    "\tresp = requests.get(url)\n",
    "\tj = json.loads(resp.text)\n",
    "\tassert resp.status_code == 200, resp.text\n",
    "\tassert j['data']['id'] == userid, resp.text\n",
    "\tassert j['data']['first_name'] == firstname, resp.text\n",
    "\n",
    "def test_list_invaliduser(supply_url):\n",
    "\turl = supply_url + \"/users/50\"\n",
    "\tresp = requests.get(url)\n",
    "\tassert resp.status_code == 404, resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370ec4e",
   "metadata": {},
   "source": [
    "**test_login_user.py** – contains test methods for testing login functionality.\n",
    "\n",
    "- **test_login_valid tests** the valid login attempt with email and password\n",
    "- **test_login_no_password** tests the invalid login attempt without passing password\n",
    "- **test_login_no_email** tests the invalid login attempt without passing email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import requests\n",
    "import json\n",
    "def test_login_valid(supply_url):\n",
    "\turl = supply_url + \"/login/\" \n",
    "\tdata = {'email':'test@test.com','password':'something'}\n",
    "\tresp = requests.post(url, data=data)\n",
    "\tj = json.loads(resp.text)\n",
    "\tassert resp.status_code == 200, resp.text\n",
    "\tassert j['token'] == \"QpwL5tke4Pnpja7X\", resp.text\n",
    "\n",
    "def test_login_no_password(supply_url):\n",
    "\turl = supply_url + \"/login/\" \n",
    "\tdata = {'email':'test@test.com'}\n",
    "\tresp = requests.post(url, data=data)\n",
    "\tj = json.loads(resp.text)\n",
    "\tassert resp.status_code == 400, resp.text\n",
    "\tassert j['error'] == \"Missing password\", resp.text\n",
    "\n",
    "def test_login_no_email(supply_url):\n",
    "\turl = supply_url + \"/login/\" \n",
    "\tdata = {}\n",
    "\tresp = requests.post(url, data=data)\n",
    "\tj = json.loads(resp.text)\n",
    "\tassert resp.status_code == 400, resp.text\n",
    "\tassert j['error'] == \"Missing email or username\", resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b720f",
   "metadata": {},
   "source": [
    "**Run:**\n",
    "\n",
    "<code>py.test -v<code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
